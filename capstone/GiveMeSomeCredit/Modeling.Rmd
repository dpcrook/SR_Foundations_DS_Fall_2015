---
title: "Modeling - GiveMeSomeCredit"
author: "David Crook"
date: "November 6, 2015"
output: html_document
fontsize: 12
geometry: margin=0.6in
---

## Setup

#### {r message=FALSE, warning=FALSE, echo=FALSE}

```{r message=FALSE, warning=FALSE, echo=FALSE}
setwd("~/projects/Classes/FoundationsOfDataScience_sliderule/github/capstone/GiveMeSomeCredit")

# Load CART packages
library(rpart)
library(rpart.plot)

suppressMessages(library(dplyr)) # summarise
library(caret)
library(data.table)


source('EvaluationMetrics.R')

# set randomizer's seed
set.seed(142)
```

## Read the data set

```{r Import data, echo=FALSE}
# read in cleaned version saved by EDA.Rmd
cs <- read.csv("cs-training-cleaned.csv")
cs.test <- read.csv("cs-test.csv")

#str(cs)
#str(cs.test)

nb_samples <- nrow(cs)
```

Out of the **`r formatC(nb_samples, format='d', big.mark=',')`** samples, the incidence of loan delinquency is **`r formatC(100 * sum(cs$SeriousDlqin2yrs == 'delinquent') / nb_samples, format='f', digits=2, big.mark=',')`%**.

## Split the data


Let's split the data into a Training set and a Test set:

```{r}
train_proportion <- .2
train_indices <- createDataPartition(
  y=cs$SeriousDlqin2yrs,
  p=train_proportion,
  list=FALSE)

cs_train <- cs[train_indices, ]
cs_test <- cs[-train_indices, ]
```

Let's also further split a bit of data from the Training set as a Validation set for the purpose of estimating Out-Of-Sample performance metrics:

```{r}
valid_proportion_of_train <- 1 / 3
valid_indices <- createDataPartition(
  y=cs_train$SeriousDlqin2yrs,
  p=valid_proportion_of_train,
  list=FALSE)

cs_valid <- cs_train[valid_indices, ]
cs_train <- cs_train[-valid_indices, ]
```

Just to check that the data was split representatively by **`createDataPartition`** from **`caret`**: the delinquency incidences in the Training, Validation and Test sets are **`r formatC(100 * sum(cs_train$SeriousDlqin2yrs == 'delinquent') / nrow(cs_train), format='f', digits=2, big.mark=',')`**, **`r formatC(100 * sum(cs_valid$SeriousDlqin2yrs == 'delinquent') / nrow(cs_valid), format='f', digits=2, big.mark=',')`** and **`r formatC(100 * sum(cs_test$SeriousDlqin2yrs == 'delinquent') / nrow(cs_test), format='f', digits=2, big.mark=',')`** respectively.

## Classification Models

Let's train 3 types of classification models: a Classification Tree, a Random Forest, and a Logistic Regression.

```{r}
caret_optimized_metric <- 'logLoss'   # equivalent to 1 / 2 of Deviance

caret_train_control <- trainControl(
  classProbs=TRUE,             # compute class probabilities
  summaryFunction=mnLogLoss,   # equivalent to 1 / 2 of Deviance
  method='repeatedcv',         # repeated Cross Validation
  number=5,                    # 5 folds
  repeats=2,                   # 2 repeats
  allowParallel=TRUE)
```

```{r message=FALSE, warning=FALSE}
B <- 600

x_vars = c(
  'RevolvingUtilizationOfUnsecuredLines',
  'age',
  'NumberOfTime30.59DaysPastDueNotWorse',
  'DebtRatio',
  'MonthlyIncome',
  'NumberOfOpenCreditLinesAndLoans',
  'NumberOfTimes90DaysLate',
  'NumberRealEstateLoansOrLines',
  'NumberOfTime60.89DaysPastDueNotWorse',
  'NumberOfDependents'
  )

# http://topepo.github.io/caret/Random_Forest.html

rf_model <- train(
  x=cs_train[, x_vars],
  y=cs_train$SeriousDlqin2yrs,
  method='rf',        # Random Forest
  metric=caret_optimized_metric,
  ntree=B,            # number of trees in the Random Forest
  nodesize=100,       # minimum node size set small enough to allow for complex trees,
                      # but not so small as to require too large B to eliminate high variance
  importance=TRUE,    # evaluate importance of predictors
  keep.inbag=TRUE,
  trControl=caret_train_control,
  tuneGrid=NULL)
```

